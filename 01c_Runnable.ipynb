{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableSequence\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GROQ_API_KEY =os.environ.get(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM=ChatGroq(model=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runnable Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 =PromptTemplate(template=\"write a joke with emojis about {topic} \",\n",
    "                       input_variables=['topic'])\n",
    "\n",
    "prompt2 = PromptTemplate(template=\"Explain the following a joke - {text}\",\n",
    "                         input_variables=['text'])\n",
    "parser=StrOutputParser()\n",
    "model=ChatGroq(model=\"llama-3.3-70b-versatile\",temperature=1)\n",
    "\n",
    "#chain = RunnableSequence(prompt1,model,parser,prompt2,model,parser)\n",
    "\n",
    "#In LCEL\n",
    "chain =prompt1 |model|parser|prompt2|model|parser\n",
    "\n",
    "res=chain.invoke({\"topic\":\"human\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A clever joke. Let's break it down:\n",
      "\n",
      "The joke starts by setting up a scenario: a human (represented by the ðŸƒâ€â™‚ï¸ emoji) brings a ladder (ðŸšª) to a party (ðŸŽ‰). This is an unusual and unexpected thing to do, which piques the listener's interest.\n",
      "\n",
      "The punchline is: \"Because they wanted to take their social life to the next level.\" Here, the phrase \"to the next level\" has a double meaning:\n",
      "\n",
      "1. **Literal meaning**: In a physical sense, a ladder allows you to climb up to a higher level or elevation.\n",
      "2. **Figurative meaning**: In a social context, \"taking something to the next level\" means to improve or elevate it, often implying an increase in status, quality, or success. In this case, it means to enhance one's social life or relationships.\n",
      "\n",
      "The humor comes from the wordplay between the literal and figurative meanings of \"to the next level.\" The joke relies on the listener being familiar with the idiomatic expression and making the mental connection between the physical ladder and the social aspiration.\n",
      "\n",
      "The ðŸ˜‚ and ðŸ¤£ emojis at the end of the joke are used to convey laughter and emphasize the humor, inviting the listener to share in the amusement.\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the two llm (tweet and linkedln)\n",
    "from langchain.schema.runnable import RunnableParallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 =PromptTemplate(template=\"Generate a tweet about {topic}\",\n",
    "                        input_variables=[\"topic\"])\n",
    "\n",
    "prompt2 = PromptTemplate(template=\"Generate a Linkedin post about {topic}\",\n",
    "                         input_variables=[\"topic\"]\n",
    "                         )\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"tweet\":RunnableSequence(prompt1,model,parser),\n",
    "    \"linkedin\":RunnableSequence(prompt2,model,parser)\n",
    "})\n",
    "\n",
    "result =parallel_chain.invoke({\"topic\":\"AI\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tweet': '\"Artificial Intelligence is revolutionizing the world. From personalized recommendations to life-saving medical diagnoses, AI is transforming the way we live and work. What\\'s the most exciting AI innovation you\\'ve seen lately? #AI #MachineLearning #FutureTech\"', 'linkedin': \"Here's a potential LinkedIn post about AI:\\n\\n**The Future of Work: Embracing the Power of AI**\\n\\nAs we continue to navigate the ever-changing landscape of technology, one thing is clear: Artificial Intelligence (AI) is revolutionizing the way we work and live.\\n\\nFrom automating routine tasks to analyzing complex data sets, AI is opening up new opportunities for innovation, efficiency, and growth. But what does this mean for us as professionals?\\n\\nFor one, it means that we need to be adaptable and willing to learn new skills. As AI takes over repetitive tasks, we'll be free to focus on higher-level thinking, creativity, and problem-solving. It's an exciting time to be in the workforce, with new possibilities emerging every day.\\n\\nSo, how can you start leveraging AI in your own career? Here are a few tips:\\n\\n **Stay curious**: Keep learning about the latest AI developments and trends.\\n **Explore new tools**: Experiment with AI-powered platforms and software to find what works best for you.\\n **Focus on human skills**: Develop your critical thinking, creativity, and communication skills to complement AI's capabilities.\\n\\nLet's work together to harness the power of AI and shape the future of work. Share your thoughts: how are you using AI in your career, and what opportunities or challenges do you see on the horizon?\\n\\n#AI #FutureOfWork #Innovation #CareerDevelopment #Technology #ProfessionalGrowth\\n\\nFeel free to customize it as per your preference and tone.\"}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tweet': '\"Starting to explore the incredible world of Generative AI and I\\'m blown away by its potential to revolutionize art, music, and writing What are some of the most exciting applications you\\'ve seen so far? #GenerativeAI #AIart #FutureOfCreativity\"',\n",
       " 'linkedin': 'Here\\'s a potential LinkedIn post about Generative AI:\\n\\n**\"The Future is Here: Unlocking the Power of Generative AI\"**\\n\\nI\\'m excited to start a conversation about one of the most transformative technologies of our time: Generative AI. This innovative field has the potential to revolutionize industries, from art and design to healthcare and beyond.\\n\\nGenerative AI models, such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), have the ability to generate new, synthetic data that\\'s often indistinguishable from the real thing. This technology is already being used to create stunning works of art, generate realistic synthetic data for training AI models, and even develop new medicines.\\n\\nBut the possibilities extend far beyond these examples. With Generative AI, we can:\\n\\n Improve patient outcomes by generating personalized treatment plans\\n Enhance customer experiences with tailored recommendations and content\\n Streamline business operations by automating routine tasks and generating insights\\n\\nAs we continue to explore the capabilities of Generative AI, I\\'d love to hear from you: What potential applications of this technology excite you the most? How do you think Generative AI will impact your industry or field?\\n\\nLet\\'s start a conversation and unlock the power of Generative AI together! #GenerativeAI #AI #Innovation #FutureOfWork\\n\\nFeel free to customize it as per your preference and style.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_chain.invoke({\"topic\":\"Generative AI which I start \"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RunnablePassthrough  \n",
    "**It is a special runnable primitive that simply returns the input as output without modifying it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough,RunnableParallel,RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 =PromptTemplate(template=\"write a joke with about {topic}\",\n",
    "                       input_variables=['topic'])\n",
    "\n",
    "prompt2 = PromptTemplate(template=\"Explain the following a joke - {text}\",\n",
    "                         input_variables=['text'])\n",
    "\n",
    "joke_gen_chain = RunnableSequence(prompt1,model,parser)\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"joke\":RunnablePassthrough(),\n",
    "    \"Explainable\":RunnableSequence(prompt2,model,parser)\n",
    "\n",
    "})\n",
    "\n",
    "fianl_chain =RunnableSequence(joke_gen_chain,parallel_chain)\n",
    "\n",
    "res=fianl_chain.invoke({\"topic\":\"cricket\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the cricket go to the doctor?\n",
      "\n",
      "Because it had a bowled-over stomach and was feeling a little \"pitch\"-sick.\n"
     ]
    }
   ],
   "source": [
    "print(res['joke'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A joke that's a perfect pitch (pun intended) for cricket enthusiasts. Let's break it down:\n",
      "\n",
      "The joke is a play on words, using cricket terminology to create a pun. Here's how it works:\n",
      "\n",
      "* \"Bowled-over\" is a phrase that can mean being completely surprised or overwhelmed, but in cricket, a bowler is a player who delivers the ball to the batsman. So, \"bowled-over stomach\" is a clever wordplay on the phrase, implying that the cricket's stomach is upset, much like a batsman might be bowled over (i.e., knocked over) by a fast ball.\n",
      "* \"Pitch-sick\" is another clever play on words. In cricket, the pitch refers to the rectangular area in the center of the field where the ball is bowled. However, \"pitch\" sounds similar to \"pitch\" as in \"feeling sick\" (nauseous). So, the joke is saying that the cricket is feeling a little unwell, like it has a stomach bug, and the word \"pitch\" is used to create a pun on the cricket term.\n",
      "\n",
      "The joke relies on a basic understanding of cricket terminology, but the wordplay is clever and amusing, making it a fun and lighthearted joke for cricket fans and non-fans alike.\n"
     ]
    }
   ],
   "source": [
    "print(res['Explainable'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runnable Lambda  \n",
    "**It is allows you to apply custom python functions within an AI pipeline.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': 'Why did the AI program go to therapy?\\n\\nBecause it was struggling to process its emotions.',\n",
       " 'word_count': 16}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "joke_gen_chain = RunnableSequence(prompt1,model,parser)\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"joke\":RunnablePassthrough(),\n",
    "    # \"word_count\":RunnableLambda(word_count)\n",
    "    \"word_count\":RunnableLambda(lambda x:len(x.split()))\n",
    "})\n",
    "\n",
    "final_chains = RunnableSequence(joke_gen_chain,parallel_chain)\n",
    "\n",
    "final_chains.invoke({\"topic\":\"AI\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RunnableBranch  \n",
    "**it is used when have the conditions. It work as the if else**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableBranch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompt = PromptTemplate(template=\"Write a detailed report on {topic}\",\n",
    "                        input_variables=[\"topic\"])\n",
    "\n",
    "\n",
    "prompt2 = PromptTemplate(template=\"Summarize the following text \\n {text}\",\n",
    "                         input_variables=['text'])\n",
    "\n",
    "report_gen = RunnableSequence(prompt,model,parser)\n",
    "\n",
    "#Create the chain if report is the > 500 words then summarize it.\n",
    "branch_chain = RunnableBranch(\n",
    "    #X is the output of report\n",
    "    (lambda x:len(x.split()) > 500,RunnableSequence(prompt2,model,parser)),# if condition\n",
    "    RunnablePassthrough() # Otherwise return the same output # else condition\n",
    ")\n",
    "\n",
    "chain = RunnableSequence(report_gen,branch_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = chain.invoke({\"topic\":\"Openai vs deepseek\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text provides a detailed comparison between OpenAI and DeepSeek, two prominent players in the natural language processing (NLP) space. Here's a summary of the key points:\n",
      "\n",
      "**Overview of OpenAI and DeepSeek:**\n",
      "\n",
      "* OpenAI is a non-profit research organization founded in 2015 by Elon Musk and others, with a focus on developing AI that benefits humanity.\n",
      "* DeepSeek is a lesser-known but rapidly growing player in the NLP space, founded in 2018, with a focus on developing efficient and scalable language models.\n",
      "\n",
      "**Key Features and Models:**\n",
      "\n",
      "* OpenAI: Developed models such as GPT-3, GPT-2, and DALL-E, which can generate human-like text, answer questions, and create images from text prompts.\n",
      "* DeepSeek: Developed models with efficient architecture, domain-specific models, and explainability features, making them suitable for industrial applications.\n",
      "\n",
      "**Comparison:**\n",
      "\n",
      "* Model architecture: OpenAI uses transformer-based models, while DeepSeek uses a combination of convolutional and recurrent neural networks.\n",
      "* Performance: OpenAI's models have achieved state-of-the-art results in NLP benchmarks, while DeepSeek's models have shown competitive performance.\n",
      "* Efficiency: DeepSeek's models are designed to be more efficient and scalable than traditional transformer-based models.\n",
      "* Explainability: DeepSeek's models provide built-in explainability features, while OpenAI's models provide some degree of interpretability.\n",
      "\n",
      "**Strengths and Weaknesses:**\n",
      "\n",
      "* OpenAI: Strengths include state-of-the-art performance, wide range of applications, and non-profit mission. Weaknesses include computational expense and limited explainability.\n",
      "* DeepSeek: Strengths include efficient architecture, explainability features, and domain-specific models. Weaknesses include limited recognition and limited range of applications.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "* The choice between OpenAI and DeepSeek depends on the specific needs and requirements of the application. If state-of-the-art performance and a wide range of applications are critical, OpenAI may be the better choice. If efficiency, explainability, and domain-specific models are more important, DeepSeek may be the better option.\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
