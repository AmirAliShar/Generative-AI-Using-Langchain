{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00ddfd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY =os.environ.get(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecdc72e",
   "metadata": {},
   "source": [
    "# LLM cache  \n",
    "a storage system that saves the inputs and outputs of previous LLM calls.\n",
    "\n",
    "So if you send the same prompt again, the system can return the cached response instantly instead of calling the model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e2067b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the LLM cache\n",
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain_core.caches import InMemoryCache\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e8c9b",
   "metadata": {},
   "source": [
    "# Rate limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60ab44a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "        requests_per_second=0.1,  # 1 request every 10s\n",
    "        check_every_n_seconds=0.1,  # Check every 100ms whether allowed to make a request\n",
    "        max_bucket_size=10,  # Controls the maximum burst size.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91e03fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gemini-2.5-flash\", \n",
    "                        model_provider=\"google_genai\",rate_limiter=rate_limiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9272a5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm doing well, thank you for asking! How about you?\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RES=model.invoke(\"Hi, How are You ?\")\n",
    "RES.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67371438",
   "metadata": {},
   "source": [
    "# Token usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e15df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm doing well, thank you for asking!\n",
      "\n",
      "How are you today?\n",
      "{'input_tokens': 7, 'output_tokens': 350, 'total_tokens': 357, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 333}, 'total_cost': 0}\n"
     ]
    }
   ],
   "source": [
    "# Callback handler\n",
    "from langchain_core.callbacks import UsageMetadataCallbackHandler\n",
    "callback=UsageMetadataCallbackHandler()\n",
    "\n",
    "res=model.invoke(\"Hi, How are You ?\",config={\"callbacks\":[callback]})\n",
    "print(res.content)\n",
    "print(res.usage_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a2ee00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
